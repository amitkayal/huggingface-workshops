{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20\n",
    "embedding_dim = 512\n",
    "dropout_prob = 0.5\n",
    "num_heads = 8\n",
    "seq_len = 5\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "input_ids = torch.randint(low=0, high=vocab_size, size=(1, seq_len))\n",
    "embeddings = embedding_layer(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Embedding??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_shape(func, shape, *args, **kwargs):\n",
    "\tassert func(*args, **kwargs).shape == shape\n",
    "expected_shape = torch.Size([1, seq_len, embedding_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled dot prod attention / Optional Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None, verbose=False):\n",
    "\t## query, key, value - bs, num_tokens, embedding_size\n",
    "\tattention_scores = torch.bmm(query, key.transpose(1, 2)) # bs, num_tokens, num_tokens\n",
    "\tattention_scores /= math.sqrt(query.shape[-1])\n",
    "\n",
    "\t# Apply mask before softmax\n",
    "\tif mask is not None:\n",
    "\t\tif verbose:\n",
    "\t\t\tprint (\"\\n-------Before-------\\n\")\n",
    "\t\t\tprint (attention_scores)\n",
    "\t\tattention_scores.masked_fill_(mask == 0, -float('inf')) # '_' means its an in-place op\n",
    "\n",
    "\t\tif verbose:\n",
    "\t\t\tprint (\"\\n-------After-------\\n\")\n",
    "\t\t\tprint (attention_scores)\n",
    "\n",
    "\tattention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "\tif verbose:\n",
    "\t\tprint (\"\\n-------Attention Weights-------\\n\")\n",
    "\t\tprint (attention_weights)\n",
    "\tassert (torch.allclose(attention_weights[0].sum(-1), torch.ones(attention_weights.shape[1])))\n",
    "\treturn torch.bmm(attention_weights, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_shape(scaled_dot_product_attention, expected_shape, embeddings, embeddings, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, mask=None):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(input_dim, embedding_dim)\n",
    "        self.key = nn.Linear(input_dim, embedding_dim)\n",
    "        self.value = nn.Linear(input_dim, embedding_dim)\n",
    "        self.mask = mask\n",
    "    \n",
    "    def forward(self, x):\n",
    "        query_embed = self.query(x)\n",
    "        key_embed = self.key(x)\n",
    "        value_embed = self.value(x)\n",
    "        \n",
    "        return scaled_dot_product_attention(query_embed, key_embed, value_embed, self.mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_shape(AttentionLayer(embedding_dim, embedding_dim).forward, expected_shape, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embedding_dim, attention_layer, mask=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        per_head_dim = embedding_dim // num_heads\n",
    "        self.attention_layers = nn.ModuleList([attention_layer(embedding_dim, per_head_dim, mask) \n",
    "                                               for _ in range(num_heads)])\n",
    "        self.lin = nn.Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "    def forward(self, x, key=None, value=None):\n",
    "        if key is not None and value is not None:\n",
    "            x = torch.cat([self.attention_layers[i](x, key, value) for i in range(self.num_heads)], dim=-1)\n",
    "        else:\n",
    "            x = torch.cat([self.attention_layers[i](x) for i in range(self.num_heads)], dim=-1)\n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_shape(MultiHeadAttention(num_heads, embedding_dim, AttentionLayer).forward, expected_shape, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position-wise FFN aka 1-D Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PFFN(nn.Module): \n",
    "\tdef __init__(self, embedding_dim, dropout_prob):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.lin1 = nn.Linear(embedding_dim, embedding_dim * 4)\n",
    "\t\tself.lin2 = nn.Linear(embedding_dim * 4, embedding_dim)\n",
    "\t\tself.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = F.gelu(self.lin1(x))\n",
    "\t\tx = self.dropout(self.lin2(x))\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_shape(PFFN(embedding_dim, dropout_prob).forward, expected_shape, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LayerNorm, Skip-connections and the Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "\tdef __init__(self, num_heads, embedding_dim, dropout_prob):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.attention = MultiHeadAttention(num_heads, embedding_dim, AttentionLayer)\n",
    "\t\tself.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "\t\tself.pffn = PFFN(embedding_dim, dropout_prob)\n",
    "\t\tself.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tattn_op = self.attention(x)\n",
    "\t\tx = x + attn_op # Skip connection\n",
    "\t\tx = self.layer_norm1(x)\n",
    "\t\tx = self.pffn(x) + x # Skip connection\n",
    "\t\treturn self.layer_norm2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_shape(TransformerEncoderLayer(num_heads, embedding_dim, dropout_prob).forward, expected_shape, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\n",
    "check_shape(scaled_dot_product_attention, expected_shape, embeddings, embeddings, embeddings, mask=mask, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_shape(MultiHeadAttention(num_heads, embedding_dim, AttentionLayer, mask).forward, expected_shape, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderAttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, mask=None):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(input_dim, embedding_dim)\n",
    "        self.key = nn.Linear(input_dim, embedding_dim)\n",
    "        self.value = nn.Linear(input_dim, embedding_dim)\n",
    "        self.mask = mask\n",
    "    \n",
    "    def forward(self, x, key, value):\n",
    "        query_embed = self.query(x)\n",
    "        key_embed = self.key(key)\n",
    "        value_embed = self.key(value)\n",
    "\n",
    "        return scaled_dot_product_attention(query_embed, key_embed, value_embed, self.mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_shape(EncoderDecoderAttentionLayer(embedding_dim, embedding_dim).forward, expected_shape,\n",
    "            embeddings, embeddings, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_shape(MultiHeadAttention(num_heads, embedding_dim, EncoderDecoderAttentionLayer, mask).forward, expected_shape, \n",
    "            x=embeddings, key=embeddings, value=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "\tdef __init__(self, num_heads, embedding_dim, dropout_prob, mask):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.masked_attention = MultiHeadAttention(num_heads, embedding_dim, AttentionLayer, mask)\n",
    "\t\tself.encoder_decoder_attention = MultiHeadAttention(num_heads, embedding_dim, EncoderDecoderAttentionLayer)\n",
    "\t\tself.pffn = PFFN(embedding_dim, dropout_prob)\n",
    "\t\tself.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "\t\tself.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "\t\tself.layer_norm3 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "\tdef forward(self, x, key, value):\n",
    "\t\tattn_op = self.masked_attention(x)\n",
    "\t\tx = x + attn_op # Skip connection\n",
    "\t\tlayer_norm_op = self.layer_norm1(x)\n",
    "\t\tenc_dec_attn_op = self.encoder_decoder_attention(layer_norm_op, key, value)\n",
    "\t\tx = enc_dec_attn_op + layer_norm_op # Skip connection\n",
    "\t\tlayer_norm_op = self.layer_norm2(x)\n",
    "\t\tx = self.pffn(x) + x # Skip connection\n",
    "\t\treturn self.layer_norm3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_shape(TransformerDecoderLayer(num_heads, embedding_dim, dropout_prob, mask).forward, expected_shape,\n",
    "            x=embeddings, key=embeddings, value=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder, Decoder and Transformer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_heads, embedding_layer, pos_embedding_layer, dropout_prob, seq_len, num_classes):\n",
    "        super().__init__()\n",
    "        embedding_dim = embedding_layer.embedding_dim\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.pos_embedding_layer = pos_embedding_layer\n",
    "        self.embedding_layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.enc = TransformerEncoderLayer(num_heads, embedding_dim, dropout_prob)\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\n",
    "        self.dec = TransformerDecoderLayer(num_heads, embedding_dim, dropout_prob, mask)\n",
    "        self.lin = nn.Linear(embedding_dim, num_classes)\n",
    "    \n",
    "    def forward(self, token_ids, labels=None):\n",
    "        embeddings = self.embedding_layer(token_ids)\n",
    "        seq_len = token_ids.shape[-1]\n",
    "        pos_embeddings = self.pos_embedding_layer(torch.arange(seq_len)).unsqueeze(0)\n",
    "        embeddings = self.embedding_layer_norm(embeddings + pos_embeddings)\n",
    "        x = self.enc(embeddings)\n",
    "        \n",
    "        if labels is not None:\n",
    "            dec_embeddings = self.embedding_layer(labels)\n",
    "            x = self.dec(dec_embeddings, x, x) # Pass op of encoder as key and value args\n",
    "            return self.lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.randint(low=0, high=vocab_size // 2, size=(1, seq_len))\n",
    "labels = torch.randint(low=vocab_size // 2, high=vocab_size, size=(1, seq_len))\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "positional_embedding_layer = nn.Embedding(seq_len, embedding_dim)\n",
    "transformer = Transformer(num_heads, embedding_layer, positional_embedding_layer, dropout_prob, seq_len, vocab_size)\n",
    "\n",
    "check_shape(transformer.forward, torch.Size([1, seq_len, vocab_size]), input_ids, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = transformer(input_ids, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[7, 1, 2, 0, 2]]), tensor([[10, 13, 11, 15, 12]]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[-1] = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 30.34it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "optimizer = optim.Adam(transformer.parameters())\n",
    "num_epochs = 100\n",
    "\n",
    "for i in tqdm(range(num_epochs)):\n",
    "    op = transformer(input_ids, labels).squeeze()\n",
    "    loss = F.cross_entropy(op, labels.squeeze())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = transformer(input_ids, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15, 15, 15, 15, 15]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op.argmax(dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO - Make sure it overfits, right now model predicts last token?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e8b7038d63dd4e2c311f66bb2cdc461873330b7f75ec6cf56c29cc17e4421480"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
